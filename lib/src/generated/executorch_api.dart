// Autogenerated from Pigeon (v17.3.0), do not edit directly.
// See also: https://pub.dev/packages/pigeon
// ignore_for_file: public_member_api_docs, non_constant_identifier_names, avoid_as, unused_import, unnecessary_parenthesis, prefer_null_aware_operators, omit_local_variable_types, unused_shown_name, unnecessary_import, no_leading_underscores_for_local_identifiers

import 'dart:async';
import 'dart:typed_data' show Float64List, Int32List, Int64List, Uint8List;

import 'package:flutter/foundation.dart' show ReadBuffer, WriteBuffer;
import 'package:flutter/services.dart';

PlatformException _createConnectionError(String channelName) {
  return PlatformException(
    code: 'channel-error',
    message: 'Unable to establish connection on channel: "$channelName".',
  );
}

List<Object?> wrapResponse({Object? result, PlatformException? error, bool empty = false}) {
  if (empty) {
    return <Object?>[];
  }
  if (error == null) {
    return <Object?>[result];
  }
  return <Object?>[error.code, error.message, error.details];
}

/// Tensor data type enumeration
enum TensorType {
  float32,
  int8,
  int32,
  uint8,
}

/// Model loading and execution states
enum ModelState {
  loading,
  ready,
  error,
  disposed,
}

/// Inference execution status
enum InferenceStatus {
  success,
  error,
  timeout,
  cancelled,
}

/// Tensor data for input/output
class TensorData {
  TensorData({
    required this.shape,
    required this.dataType,
    required this.data,
    this.name,
  });

  List<int?> shape;

  TensorType dataType;

  Uint8List data;

  String? name;

  Object encode() {
    return <Object?>[
      shape,
      dataType.index,
      data,
      name,
    ];
  }

  static TensorData decode(Object result) {
    result as List<Object?>;
    return TensorData(
      shape: (result[0] as List<Object?>?)!.cast<int?>(),
      dataType: TensorType.values[result[1]! as int],
      data: result[2]! as Uint8List,
      name: result[3] as String?,
    );
  }
}

/// Inference request parameters
class InferenceRequest {
  InferenceRequest({
    required this.modelId,
    required this.inputs,
    this.options,
    this.timeoutMs,
    this.requestId,
  });

  String modelId;

  List<TensorData?> inputs;

  Map<String?, Object?>? options;

  int? timeoutMs;

  String? requestId;

  Object encode() {
    return <Object?>[
      modelId,
      inputs,
      options,
      timeoutMs,
      requestId,
    ];
  }

  static InferenceRequest decode(Object result) {
    result as List<Object?>;
    return InferenceRequest(
      modelId: result[0]! as String,
      inputs: (result[1] as List<Object?>?)!.cast<TensorData?>(),
      options: (result[2] as Map<Object?, Object?>?)?.cast<String?, Object?>(),
      timeoutMs: result[3] as int?,
      requestId: result[4] as String?,
    );
  }
}

/// Inference execution result
class InferenceResult {
  InferenceResult({
    required this.status,
    required this.executionTimeMs,
    this.requestId,
    this.outputs,
    this.errorMessage,
    this.metadata,
  });

  InferenceStatus status;

  double executionTimeMs;

  String? requestId;

  List<TensorData?>? outputs;

  String? errorMessage;

  Map<String?, Object?>? metadata;

  Object encode() {
    return <Object?>[
      status.index,
      executionTimeMs,
      requestId,
      outputs,
      errorMessage,
      metadata,
    ];
  }

  static InferenceResult decode(Object result) {
    result as List<Object?>;
    return InferenceResult(
      status: InferenceStatus.values[result[0]! as int],
      executionTimeMs: result[1]! as double,
      requestId: result[2] as String?,
      outputs: (result[3] as List<Object?>?)?.cast<TensorData?>(),
      errorMessage: result[4] as String?,
      metadata: (result[5] as Map<Object?, Object?>?)?.cast<String?, Object?>(),
    );
  }
}

/// Model loading result
class ModelLoadResult {
  ModelLoadResult({
    required this.modelId,
    required this.state,
    this.errorMessage,
  });

  String modelId;

  ModelState state;

  String? errorMessage;

  Object encode() {
    return <Object?>[
      modelId,
      state.index,
      errorMessage,
    ];
  }

  static ModelLoadResult decode(Object result) {
    result as List<Object?>;
    return ModelLoadResult(
      modelId: result[0]! as String,
      state: ModelState.values[result[1]! as int],
      errorMessage: result[2] as String?,
    );
  }
}

class _ExecutorchHostApiCodec extends StandardMessageCodec {
  const _ExecutorchHostApiCodec();
  @override
  void writeValue(WriteBuffer buffer, Object? value) {
    if (value is InferenceRequest) {
      buffer.putUint8(128);
      writeValue(buffer, value.encode());
    } else if (value is InferenceResult) {
      buffer.putUint8(129);
      writeValue(buffer, value.encode());
    } else if (value is ModelLoadResult) {
      buffer.putUint8(130);
      writeValue(buffer, value.encode());
    } else if (value is TensorData) {
      buffer.putUint8(131);
      writeValue(buffer, value.encode());
    } else {
      super.writeValue(buffer, value);
    }
  }

  @override
  Object? readValueOfType(int type, ReadBuffer buffer) {
    switch (type) {
      case 128: 
        return InferenceRequest.decode(readValue(buffer)!);
      case 129: 
        return InferenceResult.decode(readValue(buffer)!);
      case 130: 
        return ModelLoadResult.decode(readValue(buffer)!);
      case 131: 
        return TensorData.decode(readValue(buffer)!);
      default:
        return super.readValueOfType(type, buffer);
    }
  }
}

/// Host API - Called from Dart to native platforms
/// Simplified to core operations: load, inference, dispose
class ExecutorchHostApi {
  /// Constructor for [ExecutorchHostApi].  The [binaryMessenger] named argument is
  /// available for dependency injection.  If it is left null, the default
  /// BinaryMessenger will be used which routes to the host platform.
  ExecutorchHostApi({BinaryMessenger? binaryMessenger})
      : __pigeon_binaryMessenger = binaryMessenger;
  final BinaryMessenger? __pigeon_binaryMessenger;

  static const MessageCodec<Object?> pigeonChannelCodec = _ExecutorchHostApiCodec();

  /// Load a model from the specified file path
  /// Returns a unique model ID for subsequent operations
  Future<ModelLoadResult> loadModel(String filePath) async {
    const String __pigeon_channelName = 'dev.flutter.pigeon.executorch_flutter.ExecutorchHostApi.loadModel';
    final BasicMessageChannel<Object?> __pigeon_channel = BasicMessageChannel<Object?>(
      __pigeon_channelName,
      pigeonChannelCodec,
      binaryMessenger: __pigeon_binaryMessenger,
    );
    final List<Object?>? __pigeon_replyList =
        await __pigeon_channel.send(<Object?>[filePath]) as List<Object?>?;
    if (__pigeon_replyList == null) {
      throw _createConnectionError(__pigeon_channelName);
    } else if (__pigeon_replyList.length > 1) {
      throw PlatformException(
        code: __pigeon_replyList[0]! as String,
        message: __pigeon_replyList[1] as String?,
        details: __pigeon_replyList[2],
      );
    } else if (__pigeon_replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (__pigeon_replyList[0] as ModelLoadResult?)!;
    }
  }

  /// Run inference on a loaded model
  /// Returns inference results or error information
  Future<InferenceResult> runInference(InferenceRequest request) async {
    const String __pigeon_channelName = 'dev.flutter.pigeon.executorch_flutter.ExecutorchHostApi.runInference';
    final BasicMessageChannel<Object?> __pigeon_channel = BasicMessageChannel<Object?>(
      __pigeon_channelName,
      pigeonChannelCodec,
      binaryMessenger: __pigeon_binaryMessenger,
    );
    final List<Object?>? __pigeon_replyList =
        await __pigeon_channel.send(<Object?>[request]) as List<Object?>?;
    if (__pigeon_replyList == null) {
      throw _createConnectionError(__pigeon_channelName);
    } else if (__pigeon_replyList.length > 1) {
      throw PlatformException(
        code: __pigeon_replyList[0]! as String,
        message: __pigeon_replyList[1] as String?,
        details: __pigeon_replyList[2],
      );
    } else if (__pigeon_replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (__pigeon_replyList[0] as InferenceResult?)!;
    }
  }

  /// Dispose a loaded model and free its resources
  /// User has full control over memory management
  Future<void> disposeModel(String modelId) async {
    const String __pigeon_channelName = 'dev.flutter.pigeon.executorch_flutter.ExecutorchHostApi.disposeModel';
    final BasicMessageChannel<Object?> __pigeon_channel = BasicMessageChannel<Object?>(
      __pigeon_channelName,
      pigeonChannelCodec,
      binaryMessenger: __pigeon_binaryMessenger,
    );
    final List<Object?>? __pigeon_replyList =
        await __pigeon_channel.send(<Object?>[modelId]) as List<Object?>?;
    if (__pigeon_replyList == null) {
      throw _createConnectionError(__pigeon_channelName);
    } else if (__pigeon_replyList.length > 1) {
      throw PlatformException(
        code: __pigeon_replyList[0]! as String,
        message: __pigeon_replyList[1] as String?,
        details: __pigeon_replyList[2],
      );
    } else {
      return;
    }
  }

  /// Get list of currently loaded model IDs
  Future<List<String?>> getLoadedModels() async {
    const String __pigeon_channelName = 'dev.flutter.pigeon.executorch_flutter.ExecutorchHostApi.getLoadedModels';
    final BasicMessageChannel<Object?> __pigeon_channel = BasicMessageChannel<Object?>(
      __pigeon_channelName,
      pigeonChannelCodec,
      binaryMessenger: __pigeon_binaryMessenger,
    );
    final List<Object?>? __pigeon_replyList =
        await __pigeon_channel.send(null) as List<Object?>?;
    if (__pigeon_replyList == null) {
      throw _createConnectionError(__pigeon_channelName);
    } else if (__pigeon_replyList.length > 1) {
      throw PlatformException(
        code: __pigeon_replyList[0]! as String,
        message: __pigeon_replyList[1] as String?,
        details: __pigeon_replyList[2],
      );
    } else if (__pigeon_replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (__pigeon_replyList[0] as List<Object?>?)!.cast<String?>();
    }
  }

  /// Enable or disable ExecuTorch debug logging
  /// Only works in debug builds
  Future<void> setDebugLogging(bool enabled) async {
    const String __pigeon_channelName = 'dev.flutter.pigeon.executorch_flutter.ExecutorchHostApi.setDebugLogging';
    final BasicMessageChannel<Object?> __pigeon_channel = BasicMessageChannel<Object?>(
      __pigeon_channelName,
      pigeonChannelCodec,
      binaryMessenger: __pigeon_binaryMessenger,
    );
    final List<Object?>? __pigeon_replyList =
        await __pigeon_channel.send(<Object?>[enabled]) as List<Object?>?;
    if (__pigeon_replyList == null) {
      throw _createConnectionError(__pigeon_channelName);
    } else if (__pigeon_replyList.length > 1) {
      throw PlatformException(
        code: __pigeon_replyList[0]! as String,
        message: __pigeon_replyList[1] as String?,
        details: __pigeon_replyList[2],
      );
    } else {
      return;
    }
  }
}

class _ExecutorchFlutterApiCodec extends StandardMessageCodec {
  const _ExecutorchFlutterApiCodec();
  @override
  void writeValue(WriteBuffer buffer, Object? value) {
    if (value is InferenceResult) {
      buffer.putUint8(128);
      writeValue(buffer, value.encode());
    } else if (value is TensorData) {
      buffer.putUint8(129);
      writeValue(buffer, value.encode());
    } else {
      super.writeValue(buffer, value);
    }
  }

  @override
  Object? readValueOfType(int type, ReadBuffer buffer) {
    switch (type) {
      case 128: 
        return InferenceResult.decode(readValue(buffer)!);
      case 129: 
        return TensorData.decode(readValue(buffer)!);
      default:
        return super.readValueOfType(type, buffer);
    }
  }
}

/// Flutter API - Called from native platforms to Dart (optional)
abstract class ExecutorchFlutterApi {
  static const MessageCodec<Object?> pigeonChannelCodec = _ExecutorchFlutterApiCodec();

  /// Notify Dart about model loading progress (optional)
  void onModelLoadProgress(String modelId, double progress);

  /// Notify Dart about inference completion (optional)
  void onInferenceComplete(String requestId, InferenceResult result);

  static void setup(ExecutorchFlutterApi? api, {BinaryMessenger? binaryMessenger}) {
    {
      final BasicMessageChannel<Object?> __pigeon_channel = BasicMessageChannel<Object?>(
          'dev.flutter.pigeon.executorch_flutter.ExecutorchFlutterApi.onModelLoadProgress', pigeonChannelCodec,
          binaryMessenger: binaryMessenger);
      if (api == null) {
        __pigeon_channel.setMessageHandler(null);
      } else {
        __pigeon_channel.setMessageHandler((Object? message) async {
          assert(message != null,
          'Argument for dev.flutter.pigeon.executorch_flutter.ExecutorchFlutterApi.onModelLoadProgress was null.');
          final List<Object?> args = (message as List<Object?>?)!;
          final String? arg_modelId = (args[0] as String?);
          assert(arg_modelId != null,
              'Argument for dev.flutter.pigeon.executorch_flutter.ExecutorchFlutterApi.onModelLoadProgress was null, expected non-null String.');
          final double? arg_progress = (args[1] as double?);
          assert(arg_progress != null,
              'Argument for dev.flutter.pigeon.executorch_flutter.ExecutorchFlutterApi.onModelLoadProgress was null, expected non-null double.');
          try {
            api.onModelLoadProgress(arg_modelId!, arg_progress!);
            return wrapResponse(empty: true);
          } on PlatformException catch (e) {
            return wrapResponse(error: e);
          }          catch (e) {
            return wrapResponse(error: PlatformException(code: 'error', message: e.toString()));
          }
        });
      }
    }
    {
      final BasicMessageChannel<Object?> __pigeon_channel = BasicMessageChannel<Object?>(
          'dev.flutter.pigeon.executorch_flutter.ExecutorchFlutterApi.onInferenceComplete', pigeonChannelCodec,
          binaryMessenger: binaryMessenger);
      if (api == null) {
        __pigeon_channel.setMessageHandler(null);
      } else {
        __pigeon_channel.setMessageHandler((Object? message) async {
          assert(message != null,
          'Argument for dev.flutter.pigeon.executorch_flutter.ExecutorchFlutterApi.onInferenceComplete was null.');
          final List<Object?> args = (message as List<Object?>?)!;
          final String? arg_requestId = (args[0] as String?);
          assert(arg_requestId != null,
              'Argument for dev.flutter.pigeon.executorch_flutter.ExecutorchFlutterApi.onInferenceComplete was null, expected non-null String.');
          final InferenceResult? arg_result = (args[1] as InferenceResult?);
          assert(arg_result != null,
              'Argument for dev.flutter.pigeon.executorch_flutter.ExecutorchFlutterApi.onInferenceComplete was null, expected non-null InferenceResult.');
          try {
            api.onInferenceComplete(arg_requestId!, arg_result!);
            return wrapResponse(empty: true);
          } on PlatformException catch (e) {
            return wrapResponse(error: e);
          }          catch (e) {
            return wrapResponse(error: PlatformException(code: 'error', message: e.toString()));
          }
        });
      }
    }
  }
}
