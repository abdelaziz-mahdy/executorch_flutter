# ExecuTorch Flutter - Example App Architecture

**Version**: 1.0
**Last Updated**: 2025-10-11

This document describes the architecture of the example app and provides step-by-step guides for adding new model types.

## Architecture Overview

The example app uses a **Strategy Pattern** with **Model Definitions** to support multiple model types (YOLO, MobileNet, etc.) in a unified playground. Each model is completely self-contained and knows how to process its inputs/outputs and render its results.

### Key Design Principles

1. **Model as Strategy**: Each model type is a complete strategy that encapsulates all its behavior
2. **Unified Playground**: Single screen supports all model types through polymorphism
3. **Settings Per Model**: Each model has its own settings class with type-safe defaults
4. **Processor Pattern**: Input/output processing is separate from model definition
5. **Reactive UI**: Settings changes immediately recreate processors with new configuration

---

## Core Architecture Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  UnifiedModelPlayground                â”‚
â”‚            (Single screen for all models)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ModelController                      â”‚
â”‚         (Owns model lifecycle & state)                 â”‚
â”‚  - execuTorchModel (loaded model)                     â”‚
â”‚  - definition (which model type)                       â”‚
â”‚  - settings (model-specific config)                    â”‚
â”‚  - processors (input/output strategies)                â”‚
â”‚  - camera (optional live stream)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ModelDefinition<TInput, TResult>      â”‚
â”‚         (Abstract base for all model types)            â”‚
â”‚                                                        â”‚
â”‚  Methods:                                              â”‚
â”‚  - createInputProcessor(settings)                      â”‚
â”‚  - createOutputProcessor(settings)                     â”‚
â”‚  - buildInputWidget(...)                               â”‚
â”‚  - buildResultRenderer(...)                            â”‚
â”‚  - buildSettingsWidget(...)                            â”‚
â”‚  - buildResultsDetailsSection(...)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MobileNetModelDef  â”‚         â”‚  YoloModelDef      â”‚
â”‚                    â”‚         â”‚                    â”‚
â”‚ Uses:              â”‚         â”‚ Uses:              â”‚
â”‚ - ImageFileInput   â”‚         â”‚ - ImageFileInput   â”‚
â”‚ - LiveCameraInput  â”‚         â”‚ - LiveCameraInput  â”‚
â”‚                    â”‚         â”‚                    â”‚
â”‚ Returns:           â”‚         â”‚ Returns:           â”‚
â”‚ - Classification   â”‚         â”‚ - DetectionResult  â”‚
â”‚   Result           â”‚         â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Structure

```
example/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ main.dart                              # App entry, model registry loader
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                                # Model definitions
â”‚   â”‚   â”œâ”€â”€ model_definition.dart              # Abstract base class
â”‚   â”‚   â”œâ”€â”€ model_registry.dart                # List of all available models
â”‚   â”‚   â”œâ”€â”€ model_input.dart                   # Input types (ImageFile, LiveCamera)
â”‚   â”‚   â”œâ”€â”€ model_settings.dart                # Base settings class
â”‚   â”‚   â”œâ”€â”€ mobilenet_model_definition.dart    # MobileNet implementation
â”‚   â”‚   â”œâ”€â”€ yolo_model_definition.dart         # YOLO implementation
â”‚   â”‚   â”œâ”€â”€ classification_model_settings.dart # Settings for classification
â”‚   â”‚   â””â”€â”€ yolo_model_settings.dart           # Settings for YOLO
â”‚   â”‚
â”‚   â”œâ”€â”€ processors/                            # Input/Output processing strategies
â”‚   â”‚   â”œâ”€â”€ base_processor.dart                # Abstract InputProcessor/OutputProcessor
â”‚   â”‚   â”œâ”€â”€ mobilenet_input_processor.dart     # MobileNet preprocessing
â”‚   â”‚   â”œâ”€â”€ mobilenet_output_processor.dart    # MobileNet postprocessing
â”‚   â”‚   â”œâ”€â”€ yolo_input_processor.dart          # YOLO preprocessing
â”‚   â”‚   â”œâ”€â”€ yolo_output_processor.dart         # YOLO postprocessing
â”‚   â”‚   â”œâ”€â”€ image_processor.dart               # Common image processing
â”‚   â”‚   â”œâ”€â”€ opencv/                            # OpenCV-based preprocessors
â”‚   â”‚   â”‚   â”œâ”€â”€ opencv_imagenet_preprocessor.dart
â”‚   â”‚   â”‚   â””â”€â”€ opencv_yolo_preprocessor.dart
â”‚   â”‚   â””â”€â”€ camera_image_converter.dart        # Camera frame conversion
â”‚   â”‚
â”‚   â”œâ”€â”€ renderers/                             # Result visualization
â”‚   â”‚   â””â”€â”€ screens/
â”‚   â”‚       â”œâ”€â”€ classification_renderer.dart   # Classification results
â”‚   â”‚       â””â”€â”€ yolo_renderer.dart             # Detection boxes overlay
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                              # Business logic
â”‚   â”‚   â””â”€â”€ model_controller.dart              # Central model state manager
â”‚   â”‚
â”‚   â”œâ”€â”€ screens/                               # UI screens
â”‚   â”‚   â””â”€â”€ unified_model_playground.dart      # Main playground screen
â”‚   â”‚
â”‚   â”œâ”€â”€ controllers/                           # Camera management
â”‚   â”‚   â”œâ”€â”€ camera_controller.dart             # Abstract camera interface
â”‚   â”‚   â”œâ”€â”€ platform_camera_controller.dart    # Flutter camera plugin
â”‚   â”‚   â””â”€â”€ opencv_camera_controller.dart      # OpenCV camera (desktop)
â”‚   â”‚
â”‚   â””â”€â”€ widgets/                               # Reusable UI components
â”‚       â”œâ”€â”€ image_input_widget.dart            # Image picker + camera toggle
â”‚       â””â”€â”€ performance_monitor.dart           # FPS/timing display
â”‚
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ models/                                # .pte model files
â”‚   â”‚   â”œâ”€â”€ mobilenet_v3_small_xnnpack.pte
â”‚   â”‚   â”œâ”€â”€ yolo11n_xnnpack.pte
â”‚   â”‚   â”œâ”€â”€ yolov8n_xnnpack.pte
â”‚   â”‚   â””â”€â”€ yolov5n_xnnpack.pte
â”‚   â”œâ”€â”€ imagenet_classes.txt                   # MobileNet labels
â”‚   â””â”€â”€ coco_labels.txt                        # YOLO labels
â”‚
â”œâ”€â”€ python/                                    # Model export scripts
â”‚   â”œâ”€â”€ setup_models.py                        # One-command model setup
â”‚   â”œâ”€â”€ export_mobilenet.py                    # Export MobileNet to .pte
â”‚   â”œâ”€â”€ export_yolo.py                         # Export YOLO to .pte
â”‚   â””â”€â”€ requirements.txt                       # Python dependencies
â”‚
â””â”€â”€ scripts/
    â””â”€â”€ run_integration_tests.sh               # Multi-platform testing
```

---

## How It Works: Data Flow

### 1. App Startup
```dart
main.dart
  â””â”€> ModelRegistry.loadAll()
       â””â”€> Returns list of ModelDefinitions
            â””â”€> UnifiedModelPlayground displays model selector
```

### 2. Model Selection
```dart
User selects "YOLO11 Nano"
  â””â”€> UnifiedModelPlayground._selectModel(yoloDefinition)
       â””â”€> Load model bytes from assets
       â””â”€> ExecuTorchModel.load(bytes)
       â””â”€> ModelController.create(definition, model, settings)
            â””â”€> Controller creates processors
            â””â”€> UI renders model-specific input widget
```

### 3. Running Inference (Static Image)
```dart
User selects image from gallery
  â””â”€> buildInputWidget() receives ImageFileInput
       â””â”€> ModelController.processInput(input)
            â””â”€> 1. inputProcessor.process(input) â†’ List<TensorData>
            â””â”€> 2. execuTorchModel.forward(tensors) â†’ List<TensorData>
            â””â”€> 3. outputProcessor.process(outputs) â†’ TResult
            â””â”€> 4. buildResultRenderer(input, result) â†’ Widget
```

### 4. Running Inference (Live Camera)
```dart
User toggles camera mode
  â””â”€> ModelController.enableCameraMode()
       â””â”€> Create CameraController (Platform or OpenCV)
       â””â”€> Start streaming frames
            â””â”€> For each frame:
                 â””â”€> Convert to LiveCameraInput
                 â””â”€> Same as static: preprocess â†’ forward â†’ postprocess
                 â””â”€> Update UI with result
```

### 5. Changing Settings
```dart
User adjusts confidence threshold
  â””â”€> buildSettingsWidget() â†’ onSettingsChanged(newSettings)
       â””â”€> ModelController.updateSettings(newSettings)
            â””â”€> Recreate processors with new settings
            â””â”€> Next inference uses new configuration
```

---

## Adding a New Model Type

Follow these steps to add support for a new model type (e.g., Segmentation, Pose Estimation, Text Generation).

### Step 1: Export Your Model to .pte Format

**Create Python export script** in `example/python/`:

```python
# example/python/export_segmentation.py
import torch
from executorch.exir import to_edge
import torchvision.models.segmentation as models

def export_segmentation_model():
    # Load PyTorch model
    model = models.deeplabv3_mobilenet_v3_large(pretrained=True)
    model.eval()

    # Example input
    example_input = (torch.randn(1, 3, 512, 512),)

    # Export to ExecuTorch
    edge_program = to_edge(torch.export.export(model, example_input))
    executorch_program = edge_program.to_executorch()

    # Save .pte file
    with open("../assets/models/deeplabv3_xnnpack.pte", "wb") as f:
        f.write(executorch_program.buffer)

    print("âœ… Segmentation model exported!")

if __name__ == "__main__":
    export_segmentation_model()
```

**Add to setup script** in `example/python/setup_models.py`:

```python
# Add to setup_models.py
from export_segmentation import export_segmentation_model

def setup_all_models():
    # ... existing models ...

    print("ğŸ“¦ Exporting Segmentation model...")
    export_segmentation_model()
```

**Run export**:
```bash
cd example/python
python3 setup_models.py  # Exports all models including new one
```

### Step 2: Create Model Settings Class

**Create** `example/lib/models/segmentation_model_settings.dart`:

```dart
import 'model_settings.dart';

class SegmentationModelSettings extends ModelSettings {
  double maskThreshold;
  bool showOverlay;

  SegmentationModelSettings({
    this.maskThreshold = 0.5,
    this.showOverlay = true,
    super.preprocessingProvider,
    super.cameraProvider,
    super.showPerformanceOverlay,
  });

  @override
  void reset() {
    maskThreshold = 0.5;
    showOverlay = true;
    super.reset();
  }
}
```

### Step 3: Create Result Class

**Create** `example/lib/models/segmentation_result.dart`:

```dart
class SegmentationResult {
  final List<int> maskData;      // Segmentation mask (HxW pixels)
  final int height;
  final int width;
  final List<String> classLabels; // Label for each class

  const SegmentationResult({
    required this.maskData,
    required this.height,
    required this.width,
    required this.classLabels,
  });
}
```

### Step 4: Create Input/Output Processors

**Create** `example/lib/processors/segmentation_input_processor.dart`:

```dart
import 'package:executorch_flutter/executorch_flutter.dart';
import 'base_processor.dart';
import '../models/model_input.dart';

class SegmentationInputProcessor extends InputProcessor<ModelInput> {
  final int targetWidth;
  final int targetHeight;

  const SegmentationInputProcessor({
    required this.targetWidth,
    required this.targetHeight,
  });

  @override
  Future<List<TensorData>> process(ModelInput input) async {
    // Convert image to tensor (512x512, RGB, normalized)
    // ... preprocessing logic ...

    return [tensorData];
  }
}
```

**Create** `example/lib/processors/segmentation_output_processor.dart`:

```dart
import 'package:executorch_flutter/executorch_flutter.dart';
import 'base_processor.dart';
import '../models/segmentation_result.dart';

class SegmentationOutputProcessor extends OutputProcessor<SegmentationResult> {
  final List<String> classLabels;
  final double maskThreshold;

  const SegmentationOutputProcessor({
    required this.classLabels,
    required this.maskThreshold,
  });

  @override
  Future<SegmentationResult> process(List<TensorData> outputs) async {
    // Extract mask from output tensor
    // Apply threshold
    // Return SegmentationResult

    return SegmentationResult(...);
  }
}
```

### Step 5: Create Result Renderer

**Create** `example/lib/renderers/screens/segmentation_renderer.dart`:

```dart
import 'package:flutter/material.dart';
import '../../models/model_input.dart';
import '../../models/segmentation_result.dart';

class SegmentationRenderer extends StatelessWidget {
  final ModelInput input;
  final SegmentationResult? result;

  const SegmentationRenderer({
    super.key,
    required this.input,
    required this.result,
  });

  @override
  Widget build(BuildContext context) {
    // Render image with segmentation mask overlay
    return Stack(
      children: [
        // Original image
        _buildInputImage(),

        // Segmentation mask overlay
        if (result != null) _buildMaskOverlay(result!),
      ],
    );
  }

  Widget _buildInputImage() { /* ... */ }
  Widget _buildMaskOverlay(SegmentationResult result) { /* ... */ }
}
```

### Step 6: Create Model Definition

**Create** `example/lib/models/segmentation_model_definition.dart`:

```dart
import 'package:flutter/material.dart';
import 'package:flutter/services.dart';
import 'model_definition.dart';
import 'model_input.dart';
import 'model_settings.dart';
import 'segmentation_model_settings.dart';
import 'segmentation_result.dart';
import '../processors/base_processor.dart';
import '../processors/segmentation_input_processor.dart';
import '../processors/segmentation_output_processor.dart';
import '../renderers/screens/segmentation_renderer.dart';
import '../widgets/image_input_widget.dart';

class SegmentationModelDefinition
    extends ModelDefinition<ModelInput, SegmentationResult> {

  const SegmentationModelDefinition({
    required super.name,
    required super.displayName,
    required super.description,
    required super.assetPath,
    required super.inputSize,
    required this.labelsAssetPath,
  }) : super(icon: Icons.layers);

  final String labelsAssetPath;

  // Cache for labels
  static final Map<String, List<String>> _labelsCache = {};

  Future<List<String>> _loadLabels() async {
    if (_labelsCache.containsKey(labelsAssetPath)) {
      return _labelsCache[labelsAssetPath]!;
    }

    final labelsString = await rootBundle.loadString(labelsAssetPath);
    final labels = labelsString.split('\n')
        .where((line) => line.isNotEmpty)
        .toList();

    _labelsCache[labelsAssetPath] = labels;
    return labels;
  }

  List<String> _loadLabelsSync() {
    if (_labelsCache.containsKey(labelsAssetPath)) {
      return _labelsCache[labelsAssetPath]!;
    }
    throw StateError('Labels not loaded. Call loadLabels() first.');
  }

  Future<List<String>> loadLabels() => _loadLabels();

  @override
  ModelSettings createDefaultSettings() {
    return SegmentationModelSettings();
  }

  @override
  Widget buildInputWidget({
    required BuildContext context,
    required Function(ModelInput) onInputSelected,
    VoidCallback? onCameraModeToggle,
    bool isCameraMode = false,
  }) {
    return ImageInputWidget(
      onImageSelected: (file) => onInputSelected(ImageFileInput(file)),
      onCameraModeToggle: onCameraModeToggle,
      isCameraMode: isCameraMode,
    );
  }

  @override
  InputProcessor<ModelInput> createInputProcessor(ModelSettings settings) {
    return SegmentationInputProcessor(
      targetWidth: inputSize,
      targetHeight: inputSize,
    );
  }

  @override
  OutputProcessor<SegmentationResult> createOutputProcessor(
    ModelSettings settings,
  ) {
    final segSettings = settings as SegmentationModelSettings;
    return SegmentationOutputProcessor(
      classLabels: _loadLabelsSync(),
      maskThreshold: segSettings.maskThreshold,
    );
  }

  @override
  Widget buildResultRenderer({
    required BuildContext context,
    required ModelInput input,
    required SegmentationResult? result,
  }) {
    return SegmentationRenderer(input: input, result: result);
  }

  @override
  Widget buildResultsDetailsSection({
    required BuildContext context,
    required SegmentationResult result,
    required double? processingTime,
  }) {
    // Show segmentation statistics (class distribution, etc.)
    return Column(
      children: [
        Text('Mask Size: ${result.width}x${result.height}'),
        // ... more details ...
      ],
    );
  }

  @override
  Widget buildSettingsWidget({
    required BuildContext context,
    required ModelSettings settings,
    required Function(ModelSettings) onSettingsChanged,
  }) {
    final segSettings = settings as SegmentationModelSettings;

    return Column(
      children: [
        // Mask threshold slider
        ListTile(
          title: Text('Mask Threshold'),
          subtitle: Slider(
            value: segSettings.maskThreshold,
            onChanged: (value) {
              segSettings.maskThreshold = value;
              onSettingsChanged(segSettings);
            },
          ),
        ),

        // Show overlay toggle
        SwitchListTile(
          title: Text('Show Overlay'),
          value: segSettings.showOverlay,
          onChanged: (value) {
            segSettings.showOverlay = value;
            onSettingsChanged(segSettings);
          },
        ),
      ],
    );
  }
}
```

### Step 7: Register Model in Registry

**Edit** `example/lib/models/model_registry.dart`:

```dart
import 'segmentation_model_definition.dart';

class ModelRegistry {
  static Future<List<ModelDefinition>> loadAll() async {
    return [
      // ... existing models ...

      // Segmentation Models
      const SegmentationModelDefinition(
        name: 'deeplabv3_mobilenet',
        displayName: 'DeepLabV3 MobileNet',
        description: 'Semantic segmentation model',
        assetPath: 'assets/models/deeplabv3_xnnpack.pte',
        inputSize: 512,
        labelsAssetPath: 'assets/segmentation_classes.txt',
      ),
    ];
  }
}
```

### Step 8: Add Assets

**Update** `example/pubspec.yaml`:

```yaml
flutter:
  assets:
    - assets/models/
    - assets/segmentation_classes.txt  # Add labels file
```

**Create** `example/assets/segmentation_classes.txt`:
```
background
person
car
...
```

### Step 9: Test

```bash
cd example
flutter run -d macos  # Or ios, android

# In the app:
# 1. Select "DeepLabV3 MobileNet" from dropdown
# 2. Pick an image
# 3. View segmentation mask overlay
# 4. Adjust settings (mask threshold, overlay visibility)
```

---

## Python Model Export Workflow

The `example/python/` directory contains scripts for exporting PyTorch models to ExecuTorch format.

### Directory Structure

```
example/python/
â”œâ”€â”€ setup_models.py                 # One-command setup (all models)
â”œâ”€â”€ export_mobilenet.py             # MobileNet V3 export
â”œâ”€â”€ export_yolo.py                  # YOLO export (v5, v8, v11)
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # Python setup instructions
```

### Quick Setup

```bash
cd example/python
python3 setup_models.py
```

This will:
1. Install Python dependencies (`torch`, `ultralytics`, `executorch`)
2. Export all models to `example/assets/models/`
3. Generate label files (`imagenet_classes.txt`, `coco_labels.txt`)
4. Verify all models are ready

### Manual Export

#### Exporting MobileNet

```bash
cd example/python
python3 export_mobilenet.py
```

**What it does**:
- Downloads MobileNet V3 Small (pretrained on ImageNet)
- Exports to ExecuTorch format with XNNPACK delegation
- Saves to `../assets/models/mobilenet_v3_small_xnnpack.pte`
- Generates `../assets/imagenet_classes.txt` (1000 classes)

#### Exporting YOLO

```bash
cd example/python
python3 export_yolo.py --model yolo11n  # or yolov8n, yolov5n
```

**What it does**:
- Downloads YOLO model from Ultralytics
- Exports to ExecuTorch format with XNNPACK delegation
- Saves to `../assets/models/{model}_xnnpack.pte`
- Generates `../assets/coco_labels.txt` (80 classes)

### Custom Model Export

**Template** for exporting custom models:

```python
# example/python/export_custom.py
import torch
from executorch.exir import to_edge
from executorch.exir.backend.canonical_partitioners import CanonicalPartitioner
from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner

def export_custom_model():
    # 1. Load your PyTorch model
    model = YourModel()
    model.load_state_dict(torch.load('your_model.pth'))
    model.eval()

    # 2. Create example input with correct shape
    example_input = (torch.randn(1, 3, 224, 224),)

    # 3. Export to ExecuTorch
    exported_program = torch.export.export(model, example_input)
    edge_program = to_edge(exported_program)

    # 4. Apply XNNPACK backend delegation (optional but recommended)
    edge_program = edge_program.to_backend(XnnpackPartitioner())

    # 5. Generate ExecuTorch program
    executorch_program = edge_program.to_executorch()

    # 6. Save .pte file
    output_path = "../assets/models/custom_model_xnnpack.pte"
    with open(output_path, "wb") as f:
        f.write(executorch_program.buffer)

    print(f"âœ… Model exported to {output_path}")
    print(f"   Input shape: {example_input[0].shape}")
    print(f"   Model size: {len(executorch_program.buffer) / 1024 / 1024:.2f} MB")

if __name__ == "__main__":
    export_custom_model()
```

### Troubleshooting Model Export

#### Issue: Model export fails with "operator not supported"

**Solution**: Check ExecuTorch supported operators:
```python
from executorch.exir import EdgeCompileConfig

# List unsupported ops
config = EdgeCompileConfig(_check_ir_validity=True)
edge_program = to_edge(exported_program, compile_config=config)
```

#### Issue: Model is too large

**Solutions**:
1. **Quantize to INT8**:
```python
from torch.ao.quantization import quantize_dynamic

quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
```

2. **Use smaller backbone**:
```python
# Instead of MobileNetV3 Large, use Small
model = models.mobilenet_v3_small(pretrained=True)
```

#### Issue: Inference is slow

**Solutions**:
1. **Enable XNNPACK delegation** (already shown above)
2. **Reduce input size**:
```python
# Instead of 640x640, use 320x320 for YOLO
example_input = (torch.randn(1, 3, 320, 320),)
```

---

## Best Practices

### Model Organization

1. **One definition per file**: `{model_name}_model_definition.dart`
2. **Dedicated settings class**: `{model_name}_model_settings.dart`
3. **Separate processors**: `{model_name}_input_processor.dart` and `{model_name}_output_processor.dart`
4. **Custom renderer**: `{model_name}_renderer.dart` in `renderers/screens/`

### Settings Management

1. **Extend ModelSettings**: Always inherit from base class
2. **Provide defaults**: Constructor should have sensible defaults
3. **Implement reset()**: Reset all settings to defaults
4. **Type-safe casting**: Cast `ModelSettings` to specific type in processors

### Performance Optimization

1. **Cache labels**: Load once, store in static map
2. **Reuse processors**: Controller recreates on settings change only
3. **Preload models**: Load model bytes before creating controller
4. **Dispose properly**: Always call `model.dispose()` in controller cleanup

### Testing New Models

1. **Test with static images first**: Easier to debug
2. **Verify tensor shapes**: Check `model.inputShapes` and `model.outputShapes`
3. **Test settings changes**: Ensure processors recreate correctly
4. **Test camera mode**: Verify frame processing performance

---

## Quick Reference: File Checklist

When adding a new model, create these files:

- [ ] `python/export_{model}.py` - Export script
- [ ] `lib/models/{model}_model_definition.dart` - Main definition
- [ ] `lib/models/{model}_model_settings.dart` - Settings class
- [ ] `lib/models/{model}_result.dart` - Result type
- [ ] `lib/processors/{model}_input_processor.dart` - Preprocessing
- [ ] `lib/processors/{model}_output_processor.dart` - Postprocessing
- [ ] `lib/renderers/screens/{model}_renderer.dart` - Visualization
- [ ] `assets/models/{model}_xnnpack.pte` - Model file
- [ ] `assets/{model}_labels.txt` - Class labels (if applicable)
- [ ] Update `lib/models/model_registry.dart` - Register model
- [ ] Update `pubspec.yaml` - Add assets

---

## Contact and Support

For questions about the example app architecture:
- Check this document first
- Review existing model implementations (MobileNet, YOLO)
- File issues at the package repository

**Last Updated**: 2025-10-11
**Example App Version**: 1.0
**Package Version**: 0.0.2
