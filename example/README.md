# ExecuTorch Flutter Example App

A comprehensive demonstration of the `executorch_flutter` plugin featuring:

- ğŸ¯ **Unified Model Playground** - Single interface for multiple model types
- ğŸ“¸ **Live Camera Inference** - Real-time object detection and classification
- ğŸ–¼ï¸ **Static Image Processing** - Upload and analyze images from gallery
- âš™ï¸ **Configurable Settings** - Adjust thresholds, preprocessing methods, and more
- ğŸ“Š **Performance Monitoring** - Real-time FPS and inference time tracking

## Supported Models

### Image Classification
- **MobileNet V3 Small** - Efficient ImageNet classification
- Performance: ~10-15ms inference time
- 1000 ImageNet classes

### Object Detection
- **YOLO11 Nano** - Latest YOLO architecture
- **YOLOv8 Nano** - Fast and accurate detection
- **YOLOv5 Nano** - Lightweight object detection
- Performance: ~20-50ms inference time
- 80 COCO classes

## Quick Start

### 1. Install Dependencies

```bash
# Install Flutter dependencies
flutter pub get

# Setup models (downloads and converts PyTorch models to .pte format)
cd python
python3 setup_models.py
cd ..
```

### 2. Run the App

```bash
# macOS
flutter run -d macos

# iOS (requires physical device, simulator not supported)
flutter run -d <device-id>

# Android
flutter run -d <device-id>
```

### 3. Choose a Model

1. Select a model from the dropdown (e.g., "YOLO11 Nano" or "MobileNet V3")
2. Pick an image from gallery OR enable camera mode
3. View results with bounding boxes (YOLO) or class predictions (MobileNet)

## Preprocessing Options

The example app demonstrates **three preprocessing approaches**:

### 1. GPU Preprocessing (Recommended) â­

**Hardware-accelerated preprocessing using Flutter Fragment Shaders:**

- âš¡ **Performance comparable to OpenCV** (very close on macOS)
- ğŸ“¦ **Zero dependencies** - uses native Flutter APIs
- ğŸŒ **All platforms** - mobile and desktop
- ğŸ¨ **Customizable** - write your own GLSL shaders
- ğŸ¯ **Great for real-time** - camera inference and high frame rates

**ğŸ“– [Complete GPU Preprocessing Tutorial](GPU_PREPROCESSING.md)** - Learn how to implement GPU-accelerated preprocessing with step-by-step guide and shader examples.

**Reference implementations:**
- **[lib/processors/shaders/](lib/processors/shaders/)** - GPU preprocessor implementations with README
- **[shaders/](../shaders/)** - GLSL fragment shaders (yolo_preprocess.frag, mobilenet_preprocess.frag)

### 2. OpenCV Preprocessing

**High-performance C++ library:**

- âš¡ **High performance** (very close to GPU on macOS)
- ğŸŒ **Cross-platform** - works on mobile and desktop
- ğŸ“¦ Requires `opencv_dart` package
- ğŸ”§ Advanced image processing and computer vision capabilities

### 3. CPU Preprocessing (image library)

**Pure Dart implementation:**

- â±ï¸ **Slower than GPU/OpenCV**, suitable for non-realtime use
- ğŸŒ **All platforms**
- ğŸ“¦ Uses `image` package
- ğŸ› **Best for debugging** - easier to inspect steps

**To switch preprocessing methods:**
1. Open Settings in the app
2. Select "Preprocessing Provider"
3. Choose: GPU Shader, OpenCV, or Image Library

## Project Structure

```
example/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ main.dart                          # App entry point
â”‚   â”œâ”€â”€ models/                            # Model definitions
â”‚   â”‚   â”œâ”€â”€ model_definition.dart          # Abstract base class
â”‚   â”‚   â”œâ”€â”€ model_registry.dart            # Available models list
â”‚   â”‚   â”œâ”€â”€ yolo_model_definition.dart     # YOLO implementation
â”‚   â”‚   â””â”€â”€ mobilenet_model_definition.dart # MobileNet implementation
â”‚   â”œâ”€â”€ processors/                        # Preprocessing strategies
â”‚   â”‚   â”œâ”€â”€ shaders/                       # GPU shader preprocessing
â”‚   â”‚   â”‚   â”œâ”€â”€ README.md                  # Shader preprocessing guide
â”‚   â”‚   â”‚   â”œâ”€â”€ gpu_yolo_preprocessor.dart # GPU YOLO preprocessing
â”‚   â”‚   â”‚   â””â”€â”€ gpu_mobilenet_preprocessor.dart # GPU MobileNet preprocessing
â”‚   â”‚   â”œâ”€â”€ yolo_processor.dart            # CPU YOLO preprocessing
â”‚   â”‚   â””â”€â”€ opencv/                        # OpenCV implementations
â”‚   â”œâ”€â”€ renderers/                         # Result visualization
â”‚   â”‚   â””â”€â”€ screens/
â”‚   â”‚       â”œâ”€â”€ yolo_renderer.dart         # Bounding box overlay
â”‚   â”‚       â””â”€â”€ classification_renderer.dart # Class predictions
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ model_controller.dart          # Model state management
â”‚   â””â”€â”€ screens/
â”‚       â””â”€â”€ unified_model_playground.dart  # Main playground screen
â”œâ”€â”€ shaders/                               # GPU shaders (GLSL)
â”‚   â”œâ”€â”€ yolo_preprocess.frag              # YOLO letterbox resize
â”‚   â””â”€â”€ mobilenet_preprocess.frag         # MobileNet center crop
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ models/                           # .pte model files (generated)
â”‚   â”œâ”€â”€ imagenet_classes.txt              # MobileNet labels
â”‚   â””â”€â”€ coco_labels.txt                   # YOLO labels
â””â”€â”€ python/                               # Model export scripts
    â”œâ”€â”€ setup_models.py                   # One-command setup
    â”œâ”€â”€ export_mobilenet.py               # MobileNet export
    â””â”€â”€ export_yolo.py                    # YOLO export
```

## Exporting Your Own Models

The example includes Python scripts for converting PyTorch models to ExecuTorch format:

### One-Command Setup (Recommended)

```bash
cd python
python3 setup_models.py
```

This will:
- âœ… Install all dependencies (torch, ultralytics, executorch)
- âœ… Export MobileNet V3 Small
- âœ… Export YOLO11n, YOLOv8n, YOLOv5n
- âœ… Generate label files
- âœ… Verify all models

### Manual Export

#### MobileNet
```bash
cd python
python3 export_mobilenet.py
```

#### YOLO
```bash
cd python
python3 export_yolo.py --model yolo11n  # or yolov8n, yolov5n
```

**ğŸ“– See [Python Export Scripts](python/README.md)** for detailed export instructions and custom model conversion.

## Testing

Run integration tests on all platforms:

```bash
cd example

# Test all platforms
./scripts/run_integration_tests.sh

# Test specific platform
./scripts/run_integration_tests.sh macos
./scripts/run_integration_tests.sh ios
./scripts/run_integration_tests.sh android
```

## Learn More

- **[GPU Preprocessing Tutorial](GPU_PREPROCESSING.md)** - Implement GPU-accelerated preprocessing with Fragment Shaders
- **[Main Package README](../README.md)** - Core API documentation and usage
- **[Example App Architecture](CLAUDE.md)** - Detailed architecture guide for developers

## Requirements

### Android
- Minimum SDK: API 23 (Android 6.0)
- Architecture: arm64-v8a

### iOS
- Minimum Version: iOS 17.0+
- Architecture: arm64 (physical devices only)
- âš ï¸ **Simulator NOT supported**

### macOS
- Minimum Version: macOS 12.0+ (Monterey)
- Architecture: arm64 (Apple Silicon only)
- âš ï¸ **Intel Macs NOT supported**
- âš ï¸ **Release builds NOT working** (debug builds work fine)

## Troubleshooting

### Models Not Loading

**Issue**: "Failed to load model" error

**Solution**:
```bash
# Re-export models
cd python
python3 setup_models.py

# Verify files exist
ls -lh ../assets/models/
```

### Camera Not Working

**Issue**: Black screen or no camera feed

**Solution**: Check camera permissions in device settings

### Low Frame Rates

**Issue**: FPS below 30

**Solutions**:
1. Switch to **GPU preprocessing** in settings
2. Use smaller model (e.g., YOLO11n instead of YOLO11m)
3. Reduce camera resolution
4. Run in release mode: `flutter run --release`

### iOS Build Errors

**Issue**: "requires minimum platform version 17.0"

**Solution**: Update deployment target in Xcode (see main README)

## Contributing

Contributions are welcome! When adding new models:

1. Create model definition in `lib/models/`
2. Implement preprocessors in `lib/processors/`
3. Add result renderer in `lib/renderers/screens/`
4. Register in `lib/models/model_registry.dart`
5. Add export script in `python/`

See **[Example App Architecture Guide](CLAUDE.md)** for detailed instructions.

## License

MIT License - see [LICENSE](../LICENSE) for details.

---

Built with â¤ï¸ to demonstrate the power of on-device ML with ExecuTorch and Flutter.
