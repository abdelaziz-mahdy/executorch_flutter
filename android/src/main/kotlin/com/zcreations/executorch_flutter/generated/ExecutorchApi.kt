// Autogenerated from Pigeon (v17.3.0), do not edit directly.
// See also: https://pub.dev/packages/pigeon

package com.zcreations.executorch_flutter.generated

import android.util.Log
import io.flutter.plugin.common.BasicMessageChannel
import io.flutter.plugin.common.BinaryMessenger
import io.flutter.plugin.common.MessageCodec
import io.flutter.plugin.common.StandardMessageCodec
import java.io.ByteArrayOutputStream
import java.nio.ByteBuffer

private fun wrapResult(result: Any?): List<Any?> {
  return listOf(result)
}

private fun wrapError(exception: Throwable): List<Any?> {
  if (exception is FlutterError) {
    return listOf(
      exception.code,
      exception.message,
      exception.details
    )
  } else {
    return listOf(
      exception.javaClass.simpleName,
      exception.toString(),
      "Cause: " + exception.cause + ", Stacktrace: " + Log.getStackTraceString(exception)
    )
  }
}

private fun createConnectionError(channelName: String): FlutterError {
  return FlutterError("channel-error",  "Unable to establish connection on channel: '$channelName'.", "")}

/**
 * Error class for passing custom error details to Flutter via a thrown PlatformException.
 * @property code The error code.
 * @property message The error message.
 * @property details The error details. Must be a datatype supported by the api codec.
 */
class FlutterError (
  val code: String,
  override val message: String? = null,
  val details: Any? = null
) : Throwable()

/** Tensor data type enumeration */
enum class TensorType(val raw: Int) {
  FLOAT32(0),
  INT8(1),
  INT32(2),
  UINT8(3);

  companion object {
    fun ofRaw(raw: Int): TensorType? {
      return values().firstOrNull { it.raw == raw }
    }
  }
}

/** Model loading and execution states */
enum class ModelState(val raw: Int) {
  LOADING(0),
  READY(1),
  ERROR(2),
  DISPOSED(3);

  companion object {
    fun ofRaw(raw: Int): ModelState? {
      return values().firstOrNull { it.raw == raw }
    }
  }
}

/** Inference execution status */
enum class InferenceStatus(val raw: Int) {
  SUCCESS(0),
  ERROR(1),
  TIMEOUT(2),
  CANCELLED(3);

  companion object {
    fun ofRaw(raw: Int): InferenceStatus? {
      return values().firstOrNull { it.raw == raw }
    }
  }
}

/**
 * Tensor specification for input/output requirements
 *
 * Generated class from Pigeon that represents data sent in messages.
 */
data class TensorSpec (
  val name: String,
  val shape: List<Long?>,
  val dataType: TensorType,
  val optional: Boolean,
  val validRange: List<Long?>? = null

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): TensorSpec {
      val name = list[0] as String
      val shape = list[1] as List<Long?>
      val dataType = TensorType.ofRaw(list[2] as Int)!!
      val optional = list[3] as Boolean
      val validRange = list[4] as List<Long?>?
      return TensorSpec(name, shape, dataType, optional, validRange)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      name,
      shape,
      dataType.raw,
      optional,
      validRange,
    )
  }
}

/**
 * Model metadata and capabilities
 *
 * Generated class from Pigeon that represents data sent in messages.
 */
data class ModelMetadata (
  val modelName: String,
  val version: String,
  val inputSpecs: List<TensorSpec?>,
  val outputSpecs: List<TensorSpec?>,
  val estimatedMemoryMB: Long,
  val properties: Map<String?, Any?>? = null

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): ModelMetadata {
      val modelName = list[0] as String
      val version = list[1] as String
      val inputSpecs = list[2] as List<TensorSpec?>
      val outputSpecs = list[3] as List<TensorSpec?>
      val estimatedMemoryMB = list[4].let { if (it is Int) it.toLong() else it as Long }
      val properties = list[5] as Map<String?, Any?>?
      return ModelMetadata(modelName, version, inputSpecs, outputSpecs, estimatedMemoryMB, properties)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      modelName,
      version,
      inputSpecs,
      outputSpecs,
      estimatedMemoryMB,
      properties,
    )
  }
}

/**
 * Tensor data for input/output
 *
 * Generated class from Pigeon that represents data sent in messages.
 */
data class TensorData (
  val shape: List<Long?>,
  val dataType: TensorType,
  val data: ByteArray,
  val name: String? = null

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): TensorData {
      val shape = list[0] as List<Long?>
      val dataType = TensorType.ofRaw(list[1] as Int)!!
      val data = list[2] as ByteArray
      val name = list[3] as String?
      return TensorData(shape, dataType, data, name)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      shape,
      dataType.raw,
      data,
      name,
    )
  }
}

/**
 * Inference request parameters
 *
 * Generated class from Pigeon that represents data sent in messages.
 */
data class InferenceRequest (
  val modelId: String,
  val inputs: List<TensorData?>,
  val options: Map<String?, Any?>? = null,
  val timeoutMs: Long? = null,
  val requestId: String? = null

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): InferenceRequest {
      val modelId = list[0] as String
      val inputs = list[1] as List<TensorData?>
      val options = list[2] as Map<String?, Any?>?
      val timeoutMs = list[3].let { if (it is Int) it.toLong() else it as Long? }
      val requestId = list[4] as String?
      return InferenceRequest(modelId, inputs, options, timeoutMs, requestId)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      modelId,
      inputs,
      options,
      timeoutMs,
      requestId,
    )
  }
}

/**
 * Inference execution result
 *
 * Generated class from Pigeon that represents data sent in messages.
 */
data class InferenceResult (
  val status: InferenceStatus,
  val executionTimeMs: Double,
  val requestId: String? = null,
  val outputs: List<TensorData?>? = null,
  val errorMessage: String? = null,
  val metadata: Map<String?, Any?>? = null

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): InferenceResult {
      val status = InferenceStatus.ofRaw(list[0] as Int)!!
      val executionTimeMs = list[1] as Double
      val requestId = list[2] as String?
      val outputs = list[3] as List<TensorData?>?
      val errorMessage = list[4] as String?
      val metadata = list[5] as Map<String?, Any?>?
      return InferenceResult(status, executionTimeMs, requestId, outputs, errorMessage, metadata)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      status.raw,
      executionTimeMs,
      requestId,
      outputs,
      errorMessage,
      metadata,
    )
  }
}

/**
 * Model loading result
 *
 * Generated class from Pigeon that represents data sent in messages.
 */
data class ModelLoadResult (
  val modelId: String,
  val state: ModelState,
  val metadata: ModelMetadata? = null,
  val errorMessage: String? = null

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): ModelLoadResult {
      val modelId = list[0] as String
      val state = ModelState.ofRaw(list[1] as Int)!!
      val metadata: ModelMetadata? = (list[2] as List<Any?>?)?.let {
        ModelMetadata.fromList(it)
      }
      val errorMessage = list[3] as String?
      return ModelLoadResult(modelId, state, metadata, errorMessage)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      modelId,
      state.raw,
      metadata?.toList(),
      errorMessage,
    )
  }
}

@Suppress("UNCHECKED_CAST")
private object ExecutorchHostApiCodec : StandardMessageCodec() {
  override fun readValueOfType(type: Byte, buffer: ByteBuffer): Any? {
    return when (type) {
      128.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          InferenceRequest.fromList(it)
        }
      }
      129.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          InferenceResult.fromList(it)
        }
      }
      130.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          ModelLoadResult.fromList(it)
        }
      }
      131.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          ModelMetadata.fromList(it)
        }
      }
      132.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          TensorData.fromList(it)
        }
      }
      133.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          TensorSpec.fromList(it)
        }
      }
      else -> super.readValueOfType(type, buffer)
    }
  }
  override fun writeValue(stream: ByteArrayOutputStream, value: Any?)   {
    when (value) {
      is InferenceRequest -> {
        stream.write(128)
        writeValue(stream, value.toList())
      }
      is InferenceResult -> {
        stream.write(129)
        writeValue(stream, value.toList())
      }
      is ModelLoadResult -> {
        stream.write(130)
        writeValue(stream, value.toList())
      }
      is ModelMetadata -> {
        stream.write(131)
        writeValue(stream, value.toList())
      }
      is TensorData -> {
        stream.write(132)
        writeValue(stream, value.toList())
      }
      is TensorSpec -> {
        stream.write(133)
        writeValue(stream, value.toList())
      }
      else -> super.writeValue(stream, value)
    }
  }
}

/**
 * Host API - Called from Dart to native platforms
 *
 * Generated interface from Pigeon that represents a handler of messages from Flutter.
 */
interface ExecutorchHostApi {
  /**
   * Load a model from the specified file path
   * Returns a unique model ID for subsequent operations
   */
  fun loadModel(filePath: String, callback: (Result<ModelLoadResult>) -> Unit)
  /**
   * Run inference on a loaded model
   * Returns inference results or error information
   */
  fun runInference(request: InferenceRequest, callback: (Result<InferenceResult>) -> Unit)
  /** Get metadata for a loaded model */
  fun getModelMetadata(modelId: String): ModelMetadata?
  /** Dispose a loaded model and free its resources */
  fun disposeModel(modelId: String)
  /** Get list of currently loaded model IDs */
  fun getLoadedModels(): List<String?>
  /** Check if a model is currently loaded and ready */
  fun getModelState(modelId: String): ModelState
  /**
   * Enable or disable ExecuTorch debug logging
   * Only works in debug builds
   */
  fun setDebugLogging(enabled: Boolean)

  companion object {
    /** The codec used by ExecutorchHostApi. */
    val codec: MessageCodec<Any?> by lazy {
      ExecutorchHostApiCodec
    }
    /** Sets up an instance of `ExecutorchHostApi` to handle messages through the `binaryMessenger`. */
    @Suppress("UNCHECKED_CAST")
    fun setUp(binaryMessenger: BinaryMessenger, api: ExecutorchHostApi?) {
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.executorch_flutter.ExecutorchHostApi.loadModel", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val filePathArg = args[0] as String
            api.loadModel(filePathArg) { result: Result<ModelLoadResult> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.executorch_flutter.ExecutorchHostApi.runInference", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val requestArg = args[0] as InferenceRequest
            api.runInference(requestArg) { result: Result<InferenceResult> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.executorch_flutter.ExecutorchHostApi.getModelMetadata", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val modelIdArg = args[0] as String
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.getModelMetadata(modelIdArg))
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.executorch_flutter.ExecutorchHostApi.disposeModel", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val modelIdArg = args[0] as String
            var wrapped: List<Any?>
            try {
              api.disposeModel(modelIdArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.executorch_flutter.ExecutorchHostApi.getLoadedModels", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.getLoadedModels())
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.executorch_flutter.ExecutorchHostApi.getModelState", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val modelIdArg = args[0] as String
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.getModelState(modelIdArg).raw)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.executorch_flutter.ExecutorchHostApi.setDebugLogging", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val enabledArg = args[0] as Boolean
            var wrapped: List<Any?>
            try {
              api.setDebugLogging(enabledArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
    }
  }
}
@Suppress("UNCHECKED_CAST")
private object ExecutorchFlutterApiCodec : StandardMessageCodec() {
  override fun readValueOfType(type: Byte, buffer: ByteBuffer): Any? {
    return when (type) {
      128.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          InferenceResult.fromList(it)
        }
      }
      129.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          TensorData.fromList(it)
        }
      }
      else -> super.readValueOfType(type, buffer)
    }
  }
  override fun writeValue(stream: ByteArrayOutputStream, value: Any?)   {
    when (value) {
      is InferenceResult -> {
        stream.write(128)
        writeValue(stream, value.toList())
      }
      is TensorData -> {
        stream.write(129)
        writeValue(stream, value.toList())
      }
      else -> super.writeValue(stream, value)
    }
  }
}

/**
 * Flutter API - Called from native platforms to Dart (optional)
 *
 * Generated class from Pigeon that represents Flutter messages that can be called from Kotlin.
 */
@Suppress("UNCHECKED_CAST")
class ExecutorchFlutterApi(private val binaryMessenger: BinaryMessenger) {
  companion object {
    /** The codec used by ExecutorchFlutterApi. */
    val codec: MessageCodec<Any?> by lazy {
      ExecutorchFlutterApiCodec
    }
  }
  /** Notify Dart about model loading progress (optional) */
  fun onModelLoadProgress(modelIdArg: String, progressArg: Double, callback: (Result<Unit>) -> Unit)
{
    val channelName = "dev.flutter.pigeon.executorch_flutter.ExecutorchFlutterApi.onModelLoadProgress"
    val channel = BasicMessageChannel<Any?>(binaryMessenger, channelName, codec)
    channel.send(listOf(modelIdArg, progressArg)) {
      if (it is List<*>) {
        if (it.size > 1) {
          callback(Result.failure(FlutterError(it[0] as String, it[1] as String, it[2] as String?)))
        } else {
          callback(Result.success(Unit))
        }
      } else {
        callback(Result.failure(createConnectionError(channelName)))
      } 
    }
  }
  /** Notify Dart about inference completion (optional) */
  fun onInferenceComplete(requestIdArg: String, resultArg: InferenceResult, callback: (Result<Unit>) -> Unit)
{
    val channelName = "dev.flutter.pigeon.executorch_flutter.ExecutorchFlutterApi.onInferenceComplete"
    val channel = BasicMessageChannel<Any?>(binaryMessenger, channelName, codec)
    channel.send(listOf(requestIdArg, resultArg)) {
      if (it is List<*>) {
        if (it.size > 1) {
          callback(Result.failure(FlutterError(it[0] as String, it[1] as String, it[2] as String?)))
        } else {
          callback(Result.success(Unit))
        }
      } else {
        callback(Result.failure(createConnectionError(channelName)))
      } 
    }
  }
}
